---
title: <font size="6" color="#002060">**Statistical study of Alcohol By Volume (ABV) in Craft Beer**</font>
author: "STAT 420, Summer 2018 - Final Project"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

---

---

## 1. Group Information

This study is conducted by the following members from <u>**Group #29**</u>:

 - Apoorva H Srinivasa (NetID: **apoorva6**)
 - Madhukar K P (NetID: **mk30**)
 - Nicholas Reinicke (NetID: **ndr3**)
 - Raymond Ordona (NetID: **rordona2**)

---

---


## 2. Introduction

There are many brew enthusiasts who like to brew beer at home. To make a good beer or to improve a recipe over many tries, it is essential to capture readings for several attributes during and after the brewing process which requires one to buy costly sensors or measuring gadgets. Using *Applied Statistics with R* we wanted to check if we can analyze and predict any of the attributes of the beer and hopefully suggest cost savings by eliminating any sensors or measuring gadgets for it. Or even predict any attribute of the final product in the early stages of brewing process so that any tweaks can be made to the recipe based on the knowledge of the predictions of the final product.

_**Alcohol By Volume (ABV)**_ is an important attribute of the final output from brewing process. For this project, we have taken as our main objective, to study the attributes in brewing process that affect the alcohol content in craft beers and make best **predictions** for it, essentially eliminating any sensors or measuring gadgets for same and also to get a measure of it in the early stages of brewing process without having to wait for weeks for the brewing to complete.

For this project, we are using dataset sourced from [Brewer's Friend Beer Recipes](https://www.kaggle.com/jtrofe/beer-recipes) available under Kaggle platform. This dataset contains information on approximately 75,000 different recipes of craft beer along with the readings of various sensors captured during the brewing process.

---

---


## 3. Methods

In this section we'll first load the raw data, perform necessary preprocessing and analyze each attributes in the dataset. We'll use visualizations like histogram and boxplots to explore and understand the domain and distribution of each attributes. We'll then define the methods that will be used to build the models and metrics that shall be used to evaluate the different models and to pick the best model. And finally we'll implement different models, refine them as needed and gather metrics for them.

---

### 3.1 Data Definition and Preparation

<font size="3"><u>**Data Definition: **</u></font>

The raw dataset has a total of 23 attributes. The response variable - `ABV` is a *real* variable. Below table lists all the other attributes (predictors) and their definitions. Attributes that are of no importance for statistical study and which will not be used are also indicated here. 

Now, in below table it's intuitive to notice that attributes like IDs and free-form text are straight away ignored for this study. One particular attribute which we have decided not to use and to which we would like to call upon attention is - `FG`. This attribute is captured after the brewing process and our response variable `ABV` is derived directly from it and to be inline with the objective stated in the [introduction](#introduction) section to predict `ABV` before the completion of brewing process we have decided to not use it in our prediction model.

| Sl# | Attribute | Domain | Used for Study? | Description                                                                    |
|:---:|-----------|:------:|:---------------:|--------------------------------------------------------------------------------|
| 01| `SugarScale`  |   Categorical  |  <font color="green">Yes</font>  |  Scale to determine the concentration of dissolved solids in wort  |  
| 02| `BrewMethod`  |   Categorical  |  <font color="green">Yes</font>  |  Various techniques for brewing (Ex: All Grain)  |  
| 03| `Size(L)`  |   Real  |  <font color="green">Yes</font>  |  Amount brewed for recipe listed  |  
| 04| `OG`  |   Real  |  <font color="green">Yes</font>  |  Specific gravity of wort before fermentation  |  
| 05| `IBU`  |   Real  |  <font color="green">Yes</font>  |  International Bittering Units  |  
| 06| `Color`  |   Real  |  <font color="green">Yes</font>  |  Color using Reference Method - light to dark ex. 40 = black  |  
| 07| `BoilSize`  |   Real  |  <font color="green">Yes</font>  |  Fluid at beginning of boil  |  
| 08| `BoilTime`  |   Real  |  <font color="green">Yes</font>  |  Time wort is boiled  |  
| 09| `BoilGravity`  |   Real  |  <font color="green">Yes</font>  |  Specific gravity of wort before the boil  |  
| 10| `Efficiency`  |   Real  |  <font color="green">Yes</font>  |  Beer mash extraction efficiency - extracting sugars from the grain during mash  |  
| 11| `FG`  |   Real  |  <font color="darkred">No</font>  |  Specific gravity of wort after fermentation  |  
| 12| `Name`  |   String  |  <font color="darkred">No</font>  |  Descriptive Name of Beer  |  
| 13| `URL`  |   String  |  <font color="darkred">No</font>  |  Location of recipe webpage  |  
| 14| `Style`  |   String  |  <font color="darkred">No</font>  |  Type of brew  |  
| 15| `PitchRate`  |   String  |  <font color="darkred">No</font>  |  Yeast added to fermentor (Data is not complete, has many non-numeric values)  |  
| 16| `PrimaryTemp`  |   String  |  <font color="darkred">No</font>  |  Temperature at fermenting stage (Data is not complete, has many non-numeric values)  |  
| 17| `PrimingMethod`  |   String  |  <font color="darkred">No</font>  |  Free form text detailing the method used for priming  |  
| 18| `PrimingAmount`  |   String  |  <font color="darkred">No</font>  |  Free form text indicating the amount of priming  |  
| 19| `MashThickness`  |   String  |  <font color="darkred">No</font>  |  Amount of water  (Data is not complete, has many non-numeric values)  |  
| 20| `BeerID`  |   Real  |  <font color="darkred">No</font>  |  Record ID in the dataset  |  
| 21| `StyleID`  |   Real  |  <font color="darkred">No</font>  |  Numeric ID for type of brew  |  
| 22| `UserId`  |   Real  |  <font color="darkred">No</font>  |  Unique Id of user who provided or updated the receipe  |  
 

<font size="3"><u>**Data Preparation: **</u></font>

Having gone through the definition of attributes and after cursory checks on the dataset we noticed that there are few records with nulls, `NaN`, `NA` or `N/A`. So as a next step we will pre-process the raw dataset by cleaning up unwanted columns and rows with unwanted values. 

Below are the detailed tasks performed as part of data pre-processing: 

- Read raw dataset
- Remove unwanted columns
- Remove rows with nulls/`NaN`/`NA`/`N/A`
- Rename the column name `Size(L)` to `Size` for ease of reference.
- Coerce `SugarScale` and `BrewMethod` into factor variables.
- Remove spaces in the levels of categorical attributes `SugarScale` and `BrewMethod`for easy representation in model summary (specifically interaction models).
- Coerce `BoilGravity` into a numerical attribute
- Reorder the row numbers (names)

Below is the `R` code block for the tasks(pseudocode) described above,

```{r}
# read raw dataset
beer_data_raw = read.csv('recipeData.csv')

# remove unwanted columns and rows with nulls/`NaN`/`NA`/`N/A`
beer_data = subset(beer_data_raw, 
                   subset = beer_data_raw$ABV != "N/A" & beer_data_raw$BoilGravity != "N/A",
                   select = c("ABV", "SugarScale", "BrewMethod", "Size.L.", "OG", "IBU", 
                              "Color", "BoilSize","BoilTime", "BoilGravity", "Efficiency"))

# rename column name
colnames(beer_data)[4] = "Size"

# coerce `SugarScale` and `BrewMethod` into factor variables
beer_data$SugarScale = as.factor(beer_data$SugarScale)
beer_data$BrewMethod = as.factor(beer_data$BrewMethod)

# remove spaces in the levels for factor variables
levels(beer_data$SugarScale) = gsub(" ", "", levels(beer_data$SugarScale), fixed = TRUE)
levels(beer_data$BrewMethod) = gsub(" ", "", levels(beer_data$BrewMethod), fixed = TRUE)

# coerce BoilGravity into a numerical attribute
beer_data$BoilGravity = as.numeric(beer_data$BoilGravity)

# reorder row numbers
rownames(beer_data) = c(1:nrow(beer_data))

```

Number of records in the dataset before pre-processing (raw dataset) : $`r nrow(beer_data_raw)`$

Number of records in the dataset after pre-processing (clean dataset): $`r nrow(beer_data)`$

Below are the first few records of the prepped dataset,

```{r echo = FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
kable(beer_data[1:7,]) %>%
  kable_styling(bootstrap_options = "striped", position = "left")
```

---

### 3.2 Data Exploration

In this section we examine the descriptive statistics like min, max, median, mean, etc., of numeric attributes and count of levels for categorical attributes to understand the domain, range and distribution for each. This can be helpful later in deciding if transformations need to be applied on repsone or any predictors.

Below `R` code computes the descriptive statistics described above.

```{r}
# descriptive stats for response attribute
row_names = c("Min", "1st Quartile", "Median", "Mean", "3rd Quartile", "Max")
resp_num_stats = cbind(summary(beer_data$ABV))
colnames(resp_num_stats) = c("ABV")
rownames(resp_num_stats) = row_names
#resp_num_stats


# descriptive stats for numeric predictor attributes
pred_num_col = c("Size","OG","IBU","Color","BoilSize","BoilTime", "BoilGravity","Efficiency")
pred_num_stats = data.frame(matrix(NA, ncol=length(pred_num_col), nrow=6))
colnames(pred_num_stats) = pred_num_col
rownames(pred_num_stats) = row_names
for(i in 1:length(pred_num_col)){
  pred_num_stats[,i] = as.vector(summary(beer_data[,c(pred_num_col[i])]))
}
#pred_num_stats


# descriptive stats for categorical predictor attributes
SugarScale_lvls = as.data.frame(table(beer_data$SugarScale))
colnames(SugarScale_lvls) = c("Levels", "Frequency") 
BrewMethod_lvls = as.data.frame(table(beer_data$BrewMethod))
colnames(BrewMethod_lvls) = c("Levels", "Frequency") 
#SugarScale_lvls
#BrewMethod_lvls
```

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4)
```

Below is the descriptive statistics for response attribute `ABV`, 

```{r warning=FALSE, echo=FALSE}
kable(resp_num_stats) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```


Below is the descriptive statistics for numeric predictor attributes,

```{r warning=FALSE, echo=FALSE}
library(kableExtra)
kable(pred_num_stats) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Below is the descriptive statistics for categorical attribute - `SugarScale`,
```{r warning=FALSE, echo=FALSE}
library(kableExtra)
kable(SugarScale_lvls) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Below is the descriptive statistics for categorical attribute - `BrewMethod`,
```{r warning=FALSE, echo=FALSE}
library(kableExtra)
kable(BrewMethod_lvls) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

**Inferences:**

From the above descriptive statistics we can make below inferences,

- Response attribute does not stretch over several orders of magnitude, so transformations (like log()) on it is unlikely to help.
- Similar to response attribute, none of the numeric attributes stretch over several orders of magnitude, so transformations (like log()) on them too is unlikely to help.
- Categorical attributes' levels look good and does not raise any concern.

---

### 3.3 Exploratory Visualization

In this section we'll plot histogram for each of the attributes in the prepped data to further stress the points made in preivous section and also to see if we can catch any outliers for individual attributes and consider - if or as needed - to remove them for better prediction performance. We'll also visually check collinearity by plotting a matrix of scatterplots for every pair of attributes in the preppred dataset. This will aid in dropping some of the highly collinear attributes to reduce time during model selection procedures like backward-AIC, etc.

Below is the `R` code for plotting histogram for individual attributes,

```{r}
# histogram plots
# TBD
# TBD

# matrix of scatterplots
# pairs()
# TBD
```

[graphs will follow here]

**Inferences:**

From the above visuals we can make below inferences,

- tbd tbd tbd
- tbd tbd tbd
- tbd tbd tbd

---

### 3.4 Methodology & Metrics

Described in this section are the high level steps that shall be taken to fit models and the metrics that shall be captured for each. Models will later be compared based on the metrics and a best one will be picked.

<u>**High Level Implemention Approach/Steps: **</u>

- Split data into train and test set; 20% of randomly picked data shall be set asside as test data and remaining 80% as test data. Models will be fit using train data and their performance on test data shall be captured. Metrics captured on the test data will mainly decide the best model.
- Fit different models using below approachs,
    + Use an additive model using all attributes as starting model and do backward search using AIC and BIC
    + Use a model with all two-way interactions along with their main effects as starting model and do backward search using AIC and BIC
    + Use a model with two-degree polynomials for all attributes along with their main effects as starting model and do backward search using AIC and BIC
    + Use a model with all two-way interactions and two-dgree polynomials for all attributes along with their main effects as starting model and do backward search using AIC and BIC.
- For each model capture the metrics defined in next sub-section on train and test data.
- We'll then pick the best model based on metrics and further tune or diagnose it for leverages, outliers and influencial observations.

<u>**Metrics:**</u>

Below are the metrics that shall be captured for each of the models that we fit. Argument to pick the best model will be made based on some or all of these metrics.

- RMSE on train data
- RMSE on test data
- LOOCV.RMSE
- R-squared
- Adj-R-squared
- Error percentage on train data
- Error percentage on test data
- Time taken to search the best model (in minutes)

---

### 3.5 Implementation

This section includes the detailed `R` code implementation following the approach described in previous section.

<br>

**Train and Test Split: **

First we'll split the dataset into train and test data. We'll set a seed and randomly pick 20% of data and set it asign as test data and will use rest 80% as the train data. Below is the `R` code for same.

```{r}
# set seed to project group id
set.seed(29)
beer_data_trn_idx  = sample(nrow(beer_data), size = trunc(0.80 * nrow(beer_data)))
beer_data_trn = beer_data[beer_data_trn_idx, ]
beer_data_tst = beer_data[-beer_data_trn_idx, ]

```

Number of records in the train dataset: $`r nrow(beer_data_trn)`$

Number of records in the test dataset : $`r nrow(beer_data_tst)`$

<br>

**Build generic function to fit models and compute metrics: **

Now, we'll build a generic fuction which will below tasks,

- Take as inputs,
    + `strt_model`: Starting model for search
    + `dir`: Direction of search
    + `crit`: Selection metric to be used - "AIC" or "BIC"
- Then, identify the model using the search method dicated by the input parameters.
- Finally computeS below metrics and return a named vector of these metrics.
    + `strt_model`: Starting model for search
    + `dir`: Direction of search
    + `crit`: Selection metric to be used - "AIC" or "BIC"
    
Below is the `R` code for the function just described,

```{r}
# function that takes start model and search criteria and return the best model from search along with metrics for it
func_fit_get_met = function(strt_model,
                            dir = "backward", 
                            crit = "AIC"){
  
  # identify the k value for step search
  if(crit == "AIC"){
    kk = 2
  } else if(crit == "BIC"){
    kk = log(nrow(beer_data_trn))
  } 
  
  # Perform variable selection using the critera defined in input
  strt_time = Sys.time()
  model_bst = step(strt_model, direction = dir, trace = 0, k = kk)
  end_time  = Sys.time()
  
  # compute metrics and return a named vector of metrics
  return(
    list("model_bst" = model_bst,
         "metrics_v" = c("rmse_trn"     = mean(resid(model_bst)^2),
                         "rmse_tst"     = mean((predict(model_bst, newdata = beer_data_tst) - beer_data_tst$ABV)^2),
                         "loocv_rmse"   = sqrt(mean((resid(model_bst) / (1 - hatvalues(model_bst))) ^ 2)),
                         "r2"           = summary(model_bst)$r.squared,
                         "adj_r2"       = summary(model_bst)$adj.r.squared,
                         "prct_err_trn" = (1/nrow(beer_data_trn)) * 
                                          sum( abs(predict(model_bst) - beer_data_trn$ABV) / predict(model_bst) ) * 100,
                         "prct_err_tst" = (1/nrow(beer_data_tst)) * 
                                          sum( abs(predict(model_bst, newdata = beer_data_tst) - beer_data_tst$ABV) / 
                                                 predict(model_bst, newdata = beer_data_tst) ) * 100,
                         "trn_time"     = end_time - strt_time
           )))
}
```

<br>

**Fit different starting models and search best one for each using backward AIC and BIC: **

With the generic fucntion defined, now we'll fit 4 starter models defined in the section [Methodology](#methodology-metrics) section and search two best models using the function, one with backward AIC search and another with backward BIC search. Along they way we'll also catpure the metrics returned by the function in a dataframe for present it in [results](#results) section. This will result in 8 models and metrics for each of them.

```{r}
### Define starter models ### 

# additive starter model
beer_m1_add  = lm(ABV ~ ., data = beer_data_trn)

# two-way interactions starter model
beer_m2_int2 = lm(ABV ~ (.)^2, data = beer_data_trn)

# two-degree polynomial starter model
beer_m3_ply2 = lm(ABV ~ (.) + I(Size^2) + I(OG^2) + I(IBU^2) + I(Color^2) + I(BoilSize^2) + I(BoilTime^2) +
                    I(BoilGravity^2) + I(Efficiency^2), data = beer_data_trn)

# starter model which encompasses all the effects of above model
beer_m4_all  = lm(ABV ~ (.) + (.)^2 + I(Size^2) + I(OG^2) + I(IBU^2) + I(Color^2) + I(BoilSize^2) + I(BoilTime^2) +
                    I(BoilGravity^2) + I(Efficiency^2), data = beer_data_trn)
```

```{r}
### Search best models for each using backward AIC and BIC and store the models in a list ###
beer_m1_add_bst_AIC  = func_fit_get_met(beer_m1_add, dir = "backward", crit = "AIC")
beer_m1_add_bst_BIC  = func_fit_get_met(beer_m1_add, dir = "backward", crit = "BIC")

beer_m2_int2_bst_AIC = func_fit_get_met(beer_m2_int2, dir = "backward", crit = "AIC")
beer_m2_int2_bst_BIC = func_fit_get_met(beer_m2_int2, dir = "backward", crit = "BIC")

beer_m3_ply2_bst_AIC = func_fit_get_met(beer_m3_ply2, dir = "backward", crit = "AIC")
beer_m3_ply2_bst_BIC = func_fit_get_met(beer_m3_ply2, dir = "backward", crit = "BIC")

beer_m4_all_bst_AIC  = func_fit_get_met(beer_m4_all, dir = "backward", crit = "AIC")
beer_m4_all_bst_BIC  = func_fit_get_met(beer_m4_all, dir = "backward", crit = "BIC")
```

```{r}
# create list of models for easy access later
beer_models_lst = list("beer_m1_add_bst_AIC" = beer_m1_add_bst_AIC$model_bst,
                       "beer_m1_add_bst_BIC" = beer_m1_add_bst_BIC$model_bst,
                       "beer_m2_int2_bst_AIC" = beer_m2_int2_bst_AIC$model_bst,
                       "beer_m2_int2_bst_BIC" = beer_m2_int2_bst_BIC$model_bst,
                       "beer_m3_ply2_bst_AIC" = beer_m3_ply2_bst_AIC$model_bst,
                       "beer_m3_ply2_bst_BIC" = beer_m3_ply2_bst_BIC$model_bst,
                       "beer_m4_all_bst_AIC" = beer_m4_all_bst_AIC$model_bst,
                       "beer_m4_all_bst_BIC" = beer_m4_all_bst_BIC$model_bst)

# create a dataframe containing rows of metric results for each model for easy access later
beer_models_metrics = rbind("beer_m1_add_bst_AIC" = beer_m1_add_bst_AIC$metrics_v,
                            "beer_m1_add_bst_BIC" = beer_m1_add_bst_BIC$metrics_v,
                            "beer_m2_int2_bst_AIC" = beer_m2_int2_bst_AIC$metrics_v,
                            "beer_m2_int2_bst_BIC" = beer_m2_int2_bst_BIC$metrics_v,
                            "beer_m3_ply2_bst_AIC" = beer_m3_ply2_bst_AIC$metrics_v,
                            "beer_m3_ply2_bst_BIC" = beer_m3_ply2_bst_BIC$metrics_v,
                            "beer_m4_all_bst_AIC" = beer_m4_all_bst_AIC$metrics_v,
                            "beer_m4_all_bst_BIC" = beer_m4_all_bst_BIC$metrics_v)
```

Below is the table containing metrics for all the models found from the selection procedures.

```{r echo=FALSE, warning=FALSE}
kable(beer_models_metrics) %>%
  kable_styling("striped") %>%
  row_spec(7, bold = T, color = "black", background = "#c5dfb3")
```

In this table, the best model is highlighted in green, we'll revisit this again in [results](#results) and [discussions](#discussion) section.

---

### 3.6 Model Refinement

In this section we'll try to diagnorse the influential observations for the best model that was picked in previous section and see if removing the influential observations makes any improvements in the model.

In below `R` code we are finding the influential observationsusing cooks distance. 

```{r}
# compute cooks distance and find the count of records that are highly influential
influ_cnt = sum(cooks.distance(beer_m4_all_bst_AIC$model_bst) > 4 / length(cooks.distance(beer_m4_all_bst_AIC$model_bst)))
```

We see that there are $`r influ_cnt`$ influential observations. We'll now try to fit a model without without the influential observations and see if the resulting model does any better; we'll comparae the metrics for the model before and after removing the influential observations.

```{r, warning=FALSE}
# fit starter model without the influential observations
beer_m4_all_imprv  = lm(ABV ~ (.) + (.)^2 + I(Size^2) + I(OG^2) + I(IBU^2) + I(Color^2) + I(BoilSize^2) 
                        + I(BoilTime^2) + I(BoilGravity^2) + I(Efficiency^2), 
                        data = beer_data_trn,
                        subset = cooks.distance(beer_m4_all_bst_AIC$model_bst) > 4 /
                                  length(cooks.distance(beer_m4_all_bst_AIC$model_bst)))

# do backward AIC search and find best model
beer_m4_all_bst_AIC_impv  = func_fit_get_met(beer_m4_all_imprv, dir = "backward", crit = "AIC")

# store the before and after metrics in a dataframe
metrics_bfr_afr = rbind("before" = beer_models_metrics[7,],
                        "after"  = beer_m4_all_bst_AIC_impv$metrics_v)

```

Below table indicates the metrics of the model before and after removing the influential observations. We can see that removing the influential observations didn't do any better, so we'll stick to the best model found with the influential observations. We'll revisit this again in [results](#results) and [discussions](#discussion) section. Also interested readers go check [Appendix - A](#appendix---a) for assumptions diagnostics for the best model.

```{r, echo=FALSE}
kable(metrics_bfr_afr) %>%
  kable_styling("striped") %>%
  row_spec(1, bold = T, color = "black", background = "#c5dfb3")
```
```{r echo=FALSE}
beer_model_best = beer_m4_all_bst_AIC$model_bst
#length(coef(beer_model_best))
```

Below is the summary of the best model, it has $`r length(coef(beer_model_best))`$ $\beta$ parameters!

```{r}
beer_model_best = beer_m4_all_bst_AIC$model_bst
summary(beer_model_best)
```

---

---

## 4. Results

Below is the table containing metrics for all the models found from the initial selection procedures.

<center>**Table-1**</center>

```{r echo=FALSE, warning=FALSE}
kable(beer_models_metrics) %>%
  kable_styling("striped") %>%
  row_spec(7, bold = T, color = "black", background = "#c5dfb3")
```

<br>

Below table contains metrics captured before and after removing the influential observations for the best one picked from above table.

<center>**Table-2**</center>

```{r, echo=FALSE}
kable(metrics_bfr_afr) %>%
  kable_styling("striped") %>%
  row_spec(1, bold = T, color = "black", background = "#c5dfb3")
```

<br>

Below is the Actual Vs Predicted scatter plot for the best model fit with and wihout influential observations,

<center>**Fig-1**</center>

```{r}
model_bfr = beer_m4_all_bst_AIC$model_bst
model_afr = beer_m4_all_bst_AIC_impv$model_bst

# use the above two models and plot Actual Vs Predicted side by side for them.
# TBD TBD TBD
# set echo to false later
```

---

---

## 5. Discussion

---

### 5.1 Reflection on Results

- TBD TBD TBD
- discuss table 1 resutls
- discuss over-fitting  and under fitting
- discuss table 2 results
- discuss before and after metrics and discuss influential observations
- discss Fig-1 scatter plots 


---

### 5.2 Conclusion

TBD TBD -  A final concluding statement   -    TBD TBD 

---

---


## 6. Appendix

---

### 6.1 Appendix - A

TBD TBD -   Include details on assumptions diagnostics  -    TBD TBD 

---

### 6.2 Appendix - B

**Just an extra, for fun,  what if we brew our own beer at home, but want to only serve 4-6 ABV contents for occasional beer drinkers?**

Let's see (still using model 3):

```{r eval=FALSE}

test_for_occasions = new_test_set[ which ( new_test_set$ABV <= 6 & new_test_set$ABV >=4 ),]

predicted_occasions = predict(model3, test_for_occasions, type="response")
actual_occasions = test_for_occasions$ABV

# RMSE
(rmse = sqrt(mean((predicted_occasions - actual_occasions)^2)))

```

That's so much better. We should be able to brew beer, predicting an expected alcohool content with an error of only  if we focus our beer making around the 4-6 ABV.

---

### 6.3 Appendix - C

**Can we serve drinkers with advance taste for beer?**

Here, we're assuming they're looking for stronger beer above 6 ABV perhaps. Let's see:

```{r eval=FALSE}

test_for_hard = new_test_set[ which ( new_test_set$ABV > 6 ),]

predicted_hard = predict(model3, test_for_hard, type="response")
actual_hard = test_for_hard$ABV

# RMSE
(rmse = sqrt(mean((predicted_hard - actual_hard)^2)))

```

Probably not. We will have an error rate of  - those beer drinkers will not be happy.

---

---