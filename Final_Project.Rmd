---
title: <font size="6" color="#002060">**Statistical study of Alcohol By Volume (ABV) in Craft Beer**</font>
author: "STAT 420, Summer 2018 - Final Project"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

---

---

## 1. Group Information

This study is conducted by the following members from <u>**Group #29**</u>:

 - Apoorva H Srinivasa (NetID: **apoorva6**)
 - Madhukar K P (NetID: **mk30**)
 - Nicholas Reinicke (NetID: **ndr3**)
 - Raymond Ordona (NetID: **rordona2**)

---

---


## 2. Introduction

There are many brew enthusiasts who like to brew beer at home. To make a good beer or to improve a recipe over many tries, it is essential to capture readings for several attributes during and after the brewing process which requires one to buy costly sensors or measuring gadgets. Using *Applied Statistics with R* we wanted to check if we can analyze and predict any of the attributes of the beer and hopefully suggest cost savings by eliminating any sensors or measuring gadgets for it. Or even predict any attribute of the final product in the early stages of brewing process so that any tweaks can be made to the recipe based on the knowledge of the predictions of the final product.

_**Alcohol By Volume (ABV)**_ is an important attribute of the final output from brewing process. For this project, we have taken as our main objective, to study the attributes in brewing process that affect the alcohol content in craft beers and make best **predictions** for it, essentially eliminating any sensors or measuring gadgets for same and also to get a measure of it in the early stages of brewing process without having to wait for weeks for the brewing to complete.

For this project, we are using dataset sourced from [Brewer's Friend Beer Recipes](https://www.kaggle.com/jtrofe/beer-recipes) available under Kaggle platform. This dataset contains information on approximately 75,000 different recipes of craft beer along with the readings of various sensors captured during the brewing process.

---

---


## 3. Methods

In this section we'll first load the raw data, perform necessary preprocessing and analyze each attributes in the dataset. We'll use visualizations like histogram and boxplots to explore and understand the domain and distribution of each attributes. We'll then define the methods that will be used to build the models and metrics that shall be used to evaluate the different models and to pick the best model. And finally we'll implement different models, refine them as needed and gather metrics for them.

---

### 3.1 Data Definition and Preparation

<font size="3"><u>**Data Definition: **</u></font>

The raw dataset has a total of 23 attributes. The response variable - `ABV` is a *real* variable. Below table lists all the other attributes (predictors) and their definitions. Attributes that are of no importance for statistical study and which will not be used are also indicated here. 

Now, in below table it's intuitive to notice that attributes like IDs and free-form text are straight away ignored for this study. One particular attribute which we have decided not to use and to which we would like to call upon attention is - `FG`. This attribute is captured after the brewing process and our response variable `ABV` is derived directly from it and to be inline with the objective stated in the [introduction](#introduction) section to predict `ABV` before the completion of brewing process we have decided to not use it in our prediction model.

| Sl# | Attribute | Domain | Used for Study? | Description                                                                    |
|:---:|-----------|:------:|:---------------:|--------------------------------------------------------------------------------|
| 01| `SugarScale`  |   Categorical  |  <font color="green">Yes</font>  |  Scale to determine the concentration of dissolved solids in wort  |  
| 02| `BrewMethod`  |   Categorical  |  <font color="green">Yes</font>  |  Various techniques for brewing (Ex: All Grain)  |  
| 03| `Size(L)`  |   Real  |  <font color="green">Yes</font>  |  Amount brewed for recipe listed  |  
| 04| `OG`  |   Real  |  <font color="green">Yes</font>  |  Specific gravity of wort before fermentation  |  
| 05| `IBU`  |   Real  |  <font color="green">Yes</font>  |  International Bittering Units  |  
| 06| `Color`  |   Real  |  <font color="green">Yes</font>  |  Color using Reference Method - light to dark ex. 40 = black  |  
| 07| `BoilSize`  |   Real  |  <font color="green">Yes</font>  |  Fluid at beginning of boil  |  
| 08| `BoilTime`  |   Real  |  <font color="green">Yes</font>  |  Time wort is boiled  |  
| 09| `BoilGravity`  |   Real  |  <font color="green">Yes</font>  |  Specific gravity of wort before the boil  |  
| 10| `Efficiency`  |   Real  |  <font color="green">Yes</font>  |  Beer mash extraction efficiency - extracting sugars from the grain during mash  |  
| 11| `FG`  |   Real  |  <font color="darkred">No</font>  |  Specific gravity of wort after fermentation  |  
| 12| `Name`  |   String  |  <font color="darkred">No</font>  |  Descriptive Name of Beer  |  
| 13| `URL`  |   String  |  <font color="darkred">No</font>  |  Location of recipe webpage  |  
| 14| `Style`  |   String  |  <font color="darkred">No</font>  |  Type of brew  |  
| 15| `PitchRate`  |   String  |  <font color="darkred">No</font>  |  Yeast added to fermentor (Data is not complete, has many non-numeric values)  |  
| 16| `PrimaryTemp`  |   String  |  <font color="darkred">No</font>  |  Temperature at fermenting stage (Data is not complete, has many non-numeric values)  |  
| 17| `PrimingMethod`  |   String  |  <font color="darkred">No</font>  |  Free form text detailing the method used for priming  |  
| 18| `PrimingAmount`  |   String  |  <font color="darkred">No</font>  |  Free form text indicating the amount of priming  |  
| 19| `MashThickness`  |   String  |  <font color="darkred">No</font>  |  Amount of water  (Data is not complete, has many non-numeric values)  |  
| 20| `BeerID`  |   Real  |  <font color="darkred">No</font>  |  Record ID in the dataset  |  
| 21| `StyleID`  |   Real  |  <font color="darkred">No</font>  |  Numeric ID for type of brew  |  
| 22| `UserId`  |   Real  |  <font color="darkred">No</font>  |  Unique Id of user who provided or updated the receipe  |  
 

<font size="3"><u>**Data Preparation: **</u></font>

Having gone through the definition of attributes and after cursory checks on the dataset we noticed that there are few records with nulls, `NaN`, `NA` or `N/A`. So as a next step we will pre-process the raw dataset by cleaning up unwanted columns and rows with unwanted values. 

Below are the detailed tasks performed as part of data pre-processing: 

- Read raw dataset
- Remove unwanted columns
- Remove rows with nulls/`NaN`/`NA`/`N/A`
- Rename the column name `Size(L)` to `Size` for ease of reference.
- Coerce `SugarScale` and `BrewMethod` into factor variables.
- Remove spaces in the levels of categorical attributes `SugarScale` and `BrewMethod`for easy representation in model summary (specifically interaction models).
- Coerce `BoilGravity` into a numerical attribute
- Reorder the row numbers (names)

Below is the `R` code block for the tasks(pseudocode) described above,

```{r}
# read raw dataset
beer_data_raw = read.csv('recipeData.csv')

# remove unwanted columns and rows with nulls/`NaN`/`NA`/`N/A`
beer_data = subset(beer_data_raw, 
                   subset = beer_data_raw$ABV != "N/A" & beer_data_raw$BoilGravity != "N/A",
                   select = c("ABV", "SugarScale", "BrewMethod", "Size.L.", "OG", "IBU", 
                              "Color", "BoilSize","BoilTime", "BoilGravity", "Efficiency"))

# rename column name
colnames(beer_data)[4] = "Size"

# coerce `SugarScale` and `BrewMethod` into factor variables
beer_data$SugarScale = as.factor(beer_data$SugarScale)
beer_data$BrewMethod = as.factor(beer_data$BrewMethod)

# remove spaces in the levels for factor variables
levels(beer_data$SugarScale) = gsub(" ", "", levels(beer_data$SugarScale), fixed = TRUE)
levels(beer_data$BrewMethod) = gsub(" ", "", levels(beer_data$BrewMethod), fixed = TRUE)

# coerce BoilGravity into a numerical attribute
beer_data$BoilGravity = as.numeric(beer_data$BoilGravity)

# reorder row numbers
rownames(beer_data) = c(1:nrow(beer_data))

```

Number of records in the dataset before pre-processing (raw dataset) : $`r nrow(beer_data_raw)`$

Number of records in the dataset after pre-processing (clean dataset): $`r nrow(beer_data)`$

Below are the first few records of the prepped dataset,

```{r echo = FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
kable(beer_data[1:7,]) %>%
  kable_styling(bootstrap_options = "striped", position = "left")
```

---

### 3.2 Data Exploration

In this section we examine the descriptive statistics like min, max, median, mean, etc., of numeric attributes and count of levels for categorical attributes to understand the domain, range and distribution for each. This can be helpful later in deciding if transformations need to be applied on repsone or any predictors.

Below `R` code computes the descriptive statistics described above.

```{r}
# descriptive stats for response attribute
row_names = c("Min", "1st Quartile", "Median", "Mean", "3rd Quartile", "Max")
resp_num_stats = cbind(summary(beer_data$ABV))
colnames(resp_num_stats) = c("ABV")
rownames(resp_num_stats) = row_names
#resp_num_stats


# descriptive stats for numeric predictor attributes
pred_num_col = c("Size","OG","IBU","Color","BoilSize","BoilTime", "BoilGravity","Efficiency")
pred_num_stats = data.frame(matrix(NA, ncol=length(pred_num_col), nrow=6))
colnames(pred_num_stats) = pred_num_col
rownames(pred_num_stats) = row_names
for(i in 1:length(pred_num_col)){
  pred_num_stats[,i] = as.vector(summary(beer_data[,c(pred_num_col[i])]))
}
#pred_num_stats


# descriptive stats for categorical predictor attributes
SugarScale_lvls = as.data.frame(table(beer_data$SugarScale))
colnames(SugarScale_lvls) = c("Levels", "Frequency") 
BrewMethod_lvls = as.data.frame(table(beer_data$BrewMethod))
colnames(BrewMethod_lvls) = c("Levels", "Frequency") 
#SugarScale_lvls
#BrewMethod_lvls
```

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4)
```

Below is the descriptive statistics for response attribute `ABV`, 

```{r warning=FALSE, echo=FALSE}
kable(resp_num_stats) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```


Below is the descriptive statistics for numeric predictor attributes,

```{r warning=FALSE, echo=FALSE}
library(kableExtra)
kable(pred_num_stats) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Below is the descriptive statistics for categorical attribute - `SugarScale`,
```{r warning=FALSE, echo=FALSE}
library(kableExtra)
kable(SugarScale_lvls) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

Below is the descriptive statistics for categorical attribute - `BrewMethod`,
```{r warning=FALSE, echo=FALSE}
library(kableExtra)
kable(BrewMethod_lvls) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left")
```

**Inferences:**

From the above descriptive statistics we can make below inferences,

- Response attribute does not stretch over several orders of magnitude, so transformations (like log()) on it is unlikely to help.
- Similar to response attribute, none of the numeric attributes stretch over several orders of magnitude, so transformations (like log()) on them too is unlikely to help.
- Categorical attributes' levels look good and does not raise any concern.

---

### 3.3 Exploratory Visualization

In this section we'll plot histogram for each of the attributes in the prepped data to further stress the points made in preivous section and also to see if we can catch any outliers for individual attributes and consider - if or as needed - to remove them for better prediction performance. We'll also visually check collinearity by plotting a matrix of scatterplots for every pair of attributes in the preppred dataset. This will aid in dropping some of the highly collinear attributes to reduce time during model selection procedures like backward-AIC, etc.

Below is the `R` code for plotting histogram for individual attributes,

```{r}
# histogram plots
# TBD
# TBD

# matrix of scatterplots
# pairs()
# TBD
```

[graphs will follow here]

**Inferences:**

From the above visuals we can make below inferences,

- tbd tbd tbd
- tbd tbd tbd
- tbd tbd tbd

---

### 3.4 Methodology & Metrics

Described in this section are the high level steps that shall be taken to fit models and the metrics that shall be captured for each. Models will later be compared based on the metrics and a best one will be picked.

<u>**High Level Implemention Approach/Steps: **</u>

- Split data into train and test set; 20% of randomly picked data shall be set asside as test data and remaining 80% as test data. Models will be fit using train data and their performance on test data shall be captured. Metrics captured on the test data will mainly decide the best model.
- Fit different models using below approachs,
    + Use an additive model using all attributes as starting model and do backward search using AIC and BIC
    + Use a model with all two-way interactions along with their main effects as starting model and do backward search using AIC and BIC
    + Use a model with all three-way interactions along with their main effects as starting model and do backward search using AIC and BIC
    + Use a model with two-degree polynomials for all attributes along with their main effects as starting model and do backward search using AIC and BIC
    + Use a model with all two-way interactions and two-degree polynomials for all attributes along with their main effects as starting model and do backward search using AIC and BIC.
- For each model capture the metrics defined in next sub-section on train and test data.

<u>**Metrics:**</u>

- RMSE on train data
- RMSE on test data
- LOOCV.RMSE
- R-squared
- Adj-R-squared
- Error pecentage




---

### 3.5 Implementation

*[Notes: Split the data into train and test set. Apply all the methods and capture the metrics for each model on train and test dataset, have these stored in a final dataframe.]*

The following helper function (**residual_fitted_plot**) is used to plot residual-vs-fitted and outliers.  This function accepts model, data, outlier, and threshold as input parameters. The outlier parameter is BOOLEAN. If set to TRUE, it will use the threshold parameter to compute and plot for the outliers. The function will return the data set ( without the outliers if the outlier parameter and threshold input are used).

```{r, echo = FALSE}
library("leaps")
options(scipen = 1, digits = 3, width = 80, fig.align = "center")
```

```{r}

beer = read.csv('recipeData3.csv')
beer$Style = as.factor(beer$Style)

coln = c('OG', 'FG', 'ABV', 'IBU', 'Color', 'Efficiency',
         'BoilGravity',  'BoilTime', 'SugarScale', 'BrewMethod')


dataset = subset( beer[, coln], !BoilGravity %in% c("NA","N/A" ) & !ABV %in% c("NA","N/A"))
dataset[, c("BoilGravity")] = as.numeric(dataset[, c("BoilGravity")])


set.seed(142)
n = nrow(dataset)
ind = sample(1:nrow(dataset), n * 9/10, replace = FALSE, prob = NULL)

train_set = dataset[ind,]
test_set = dataset[-ind,]

nrow(beer)
nrow(train_set)
nrow(test_set)

```


```{r}

# This function plots the residual-to-fitted chart. 
# If outlier is set to TRUE, it will return dataset without the outliers;
# otherwise, it returns the complete dataset.
residual_fitted_plot = function(model, data,  outlier, threshold = 0) {
  
  plot(fitted(model), resid(model), xlab="Fitted Values", ylab="Residual", 
       main="Alcohol By Volume (Residual-vs-Fitted Plot)", col="dodgerblue")
  abline(h=0,  col = "darkorange", lwd=2)

  if (outlier == TRUE ) {
    # Plotting outliers
    outliers = resid(model) < threshold
    outlier_index = which( outliers )
    fitted_outliers = fitted(model)[outlier_index]
    resid_outliers = resid(model)[outlier_index]
    
    points(fitted_outliers, resid_outliers, lwd=2, col="red")
    text(fitted_outliers, resid_outliers, labels = round(outlier_index, 3), pos = 2)
    
    #Return dataset without the outliers
    return ( data[-outlier_index,] )
    
  }
  
  return ( data )
}

```

The following helper function (**result_metrics**) is used to capture R2, adjusted R2, RMSE, number of coefficients used, and  list of coefficients that are significant based of a given alpha.

```{r}

# This function returns numeric results (R2, adjusted R3, RMSE, no of coefficients, no of
# non-significant coefficients.)
result_metrics = function(model, alpha) {
  # Get R2  
  model_r2 = round( summary(model)$r.squared, 2)
  # Get Adjusted R2
  model_adj_r2 = round( summary(model)$adj.r.squared, 2)
  # Get RMSE
  model_rmse = round( sqrt(mean(resid(model)^2)), 2 )
  # Get number of coefficients that are significant at .1
  coef = summary(model)$coefficients
  model_coef_cnt = nrow(coef) - 1
  model_not_signif_coef = names( which( coef[,"Pr(>|t|)"] > alpha ) )
  model_not_sigif_coef_cnt = length(model_not_signif_coef)
  list( "r2" = model_r2,
        "adjusted_r2" = model_adj_r2,
        "rmse" = model_rmse,
        "coef_count" = model_coef_cnt,
        "not_sig_coef_count" = model_not_sigif_coef_cnt,
        "not_sig_coef" = model_not_signif_coef )
}
```


---

#### Study 1: Using AIC backward Stepwise Search with additive model and two-way interaction across the additive model.

In this study 1, we have captured the time it takes to capture the search.

```{r}


start_model = lm(ABV ~ (. -FG - IBU)^2 , data = train_set)

system.time((
model1_srch = step( start_model, direction = "backward", trace=0) 
))

```

Note: running the search takes approximately between 27 seconds to 2 minutes.

**Plotting and Validating outliers:**

```{r}
# Plot residual-fitted, and return dataset without outliers.
data_no_outliers =  residual_fitted_plot(model1_srch, train_set, TRUE, -9) 
```

Note that there are 5 observed outliers out of 63.7k observations. By removing the outliers, the residual range narrowed from (-19,6) to (-6,6). See the new plot in Appendix section.

Refit a model againt dataset with no outliers, using the model derived from the aic backward stepwise search.

```{r}
model1  =  lm(formula = ABV ~ OG + Color + Efficiency + BoilGravity + BoilTime + 
    SugarScale + BrewMethod + OG:Efficiency + OG:BoilTime + OG:SugarScale + 
    OG:BrewMethod + Color:Efficiency + Color:BoilGravity + Color:BoilTime + 
    Color:SugarScale + Color:BrewMethod + Efficiency:BoilTime + 
    Efficiency:SugarScale + Efficiency:BrewMethod + BoilGravity:BoilTime + 
    BoilGravity:BrewMethod + BoilTime:SugarScale + BoilTime:BrewMethod + 
    SugarScale:BrewMethod, 
    data = data_no_outliers)

model1_result = result_metrics(model1, alpha=0.10)
model1_call = model1$call
model1_result
```

---

#### Study 2: Using AIC backward Stepwise Search with additive model and two-way interaction for only the categorical predictors.

```{r}

start_model = lm(ABV ~ (. - FG - IBU ) + 
        (OG + Color + Efficiency + BoilGravity + BoilTime ) * (SugarScale + BrewMethod),
         data = train_set)

system.time((
model2_srch = step( start_model, direction = "backward", trace=0) 
))

```

Note: running the search takes approximately between 2.829 seconds to 5 seconds.

**Plotting and Validating outliers:**

```{r}
# Plot residual-fitted, and return dataset without outliers.
data_no_outliers =  residual_fitted_plot(model2_srch, train_set, TRUE, -9) 
```

Note that there are 5 observed outliers out of 63.7k observations. By removing the outliers, the residual range narrowed from (-19,6) to (-6,6). See the new plot in Appendix section.

Refit a model againt dataset with no outliers, using the model derived from the aic backward stepwise search.

```{r}
model2 =   lm(formula = ABV ~ OG + Color + Efficiency + BoilGravity + BoilTime + 
    SugarScale + BrewMethod + OG:SugarScale + OG:BrewMethod + 
    Color:SugarScale + Efficiency:BrewMethod + BoilGravity:SugarScale + 
    BoilGravity:BrewMethod + BoilTime:SugarScale + BoilTime:BrewMethod, 
    data = data_no_outliers)

model2_result = result_metrics(model2, alpha=0.10)
model2_call = model2$call
model2_result
```


---

#### Study 3: Using Exhaustive Search and Parameterization.

```{r}

system.time( (
start_model = summary(regsubsets(ABV ~ (. -FG - IBU )^2 , data = train_set, really.big=TRUE))
)
)

# Derive coefficients/variables based on the highest Adjusted R-squared.
best_r2_ind = which.max(start_model$adjr2)
coef = names( which(start_model$which[best_r2_ind, ]) )
cat("Chosen variables by Exhaustive Search:\n")
coef
# Parameterization
# Using the coefficients/variables derived from the exhaustive search, create a new dataframe using
# three new dummy variables for both train_set and test_set

new_train_set = data.frame(
ABV = train_set$ABV,
OG = train_set$OG,
Color = train_set$Color,
BoilGravity = train_set$BoilGravity,
BoilTime = train_set$BoilTime,
v1 = 1 * as.numeric(train_set$SugarScale == "Specific Gravity"),
v2 = 1 * as.numeric(train_set$BrewMethod == "BIAB"),
v3 = 1 * as.numeric(train_set$BrewMethod == "extract")
)

new_test_set = data.frame(
ABV = test_set$ABV,
OG = test_set$OG,
Color = test_set$Color,
BoilGravity = test_set$BoilGravity,
BoilTime = test_set$BoilTime,
v1 = 1 * as.numeric(test_set$SugarScale == "Specific Gravity"),
v2 = 1 * as.numeric(test_set$BrewMethod == "BIAB"),
v3 = 1 * as.numeric(test_set$BrewMethod == "extract")
)


# Now try to fit a model
model_exhaust = lm( ABV ~ OG + Color + v1 + v2 + v3 + OG:v1 + Color:BoilTime +  BoilTime:v3,
              data = new_train_set)


```

Note: running the search takes approximately between 0.957 seconds to 5 seconds.

**Plotting and Validating outliers:**

```{r}
# Plot residual-fitted, and return dataset without outliers.
data_no_outliers =  residual_fitted_plot(model_exhaust, new_train_set, TRUE, -9) 
```

Note that there are 5 observed outliers out of 63.7k observations. By removing the outliers, the residual range narrowed from (-19,6) to (-6,6). See the new plot in Appendix section.

Refit a model againt dataset with no outliers, using the model derived from the exhaustive search.

```{r}

model3 = lm( ABV ~ OG + Color + v1 + v2 + v3 + OG:v1 + Color:BoilTime +  BoilTime:v3,
              data = data_no_outliers)

model3_result = result_metrics(model3, alpha=0.10)
model3_call = model3$call
model3_result
```



---

### 3.6 Model(s) Refinement

*Notes: See if any refinements are possible, example check if removing outliers improves the models performance on test dataset]*

---

## 4. Results

*[In this section we'll provide metrics table and any visualizations on the metrics]*


*Summary:* Model 3 is chosen to be the best.  Explained below ...

This study has collected numerical results for R2, adjusted R2, RMSE, number of significant coefficients for comparison. Below is a table listing the results of the three studies:

```{r}
results = NULL
results = rbind(results, model1_result)
results = rbind(results, model2_result)
results = rbind(results, model3_result)
results = as.matrix( results )
results1 = results[, -c(6)]
dframe_results = data.frame(results1)
rownames(dframe_results) = c("Study 1", "Study 2", "Study 3")
knitr::kable(dframe_results, type="markdown")
```

We have shown in our study three good models with R-squared at (`r model3_result$r2`) and 
adjusted-R2 (`r model3_result$adjusted_r2`) and an RMSE (`r model1_result$rmse`). The only difference is the number of variables/coefficients ( showing both significant and non-significant p-values ).
From the  perspective of significance of regression,  it does show that model 3 has the more simple and less variables than the other two models. However, model 1 seems to have a more dominant significant p-values - this may be true based on the idea that the more predictors/variables we have, the more degrees of freedom, and thus, the higher towards fitting a better model.

To prove the point, what does ANOVA tell us in the F-test?

First, we compared model1 and model2.  Model2 being the Null Hypothesis.

```{r}
anova( model1_srch, model2_srch)
```

*Note that model1_srch is model 1 in anova,  and model2_srch is model 2 in anova.  Model 2 is the Null Hypothesis*

The result of the F-TEST does show that model 1 (Full Hypothesis) is the choice of model since the F-test rejects the null hypothesis. 

Using model 1, we compare that againt model 3 next.

```{r}
anova(model1_srch, model_exhaust)
```

*Note that model1_srch is model 1 in anova,  and model_exhaust is model 2 in anova.  Model 2 is the Null Hypothesis*

The result of the F-TEST does show that model 1 (Full Hypothesis) is the choice of model since the F-test rejects the null hypothesis. However, if one has to compare all the numeric values, apart from model 1 being slow in aic backward keywise search, there is really not much significant difference in terms of R2, adjusted R3, and RMSE. In fact, in can be argued that model 1 may have been chosen due to the fact that model 1 has more significant coefficients (`r model1_result$coef_count - model1_result$not_sig_coef_count`) than model 2 (`r model2_result$coef_count - model2_result$not_sig_coef_count`) or 3 (`r model3_result$coef_count - model3_result$not_sig_coef_count`)

So why model 3 seems to be more preferred when R2, adjusted R3, rmse are all the same among model1,2,3 - and model 1 is chosen by ANOVA? This comes down to the goal of this study which is about COST. Without compromising accuracy/measurements, if we can make beer using the least gadgets and sensors (or categorical choices even), that would be ideal. Basically, why complicate if we can get the same results with less cost. (Ref 17.3 of text).

---

### 4.1 Statistical Summary

Provide statistical summaries here and detail them.

---

### 4.2 Graphical Summary

Provide Graphical summaries here and detail them.

---

## 5. Discussion

---

### 5.1 Reflection on Results

*[Notes: Discuss the results here suggest future enhancements]*

We used new_test_set to predict ABV using model3 ( choice of model) using alpha = 0.01.

```{r}

predicted_interval = predict(model3, new_test_set, interval="prediction", level=0.01)
actual_abv = new_test_set$ABV
predicted_abv = predicted_interval[,"fit"]

abv = seq(0,15, 1)
par(mfrow=c(2,2))
hist(predicted_abv, breaks=100, xlim=range(0,15), xlab="Predicted Value from Model 3", 
     main="ABV Distribution (Predicted)")
hist(actual_abv,  breaks=100, xlim=range(0,15), xlab="Actual value from Test Set", 
     main="ABV Distribution (Actual")
```

Histogram shows similarity between predicted value and actual value. Occasional drinkers most likely prefer ABV between 4-6 which is exactly where the highest frequency falls;  and the frequency pattern of the predicted values fall in the same range.

Additionally, sampling the first 20 ABVs of both actual and predict are very close. 

```{r}
head(actual_abv, 20)
round(as.vector(head(predicted_abv, 20)),2)
```

Looking at predicted values vs actual values

```{r}
plot(predicted_abv, actual_abv, col="dodgerblue", ylab="Actual", 
      xlab="Predicted", main="Actual vs Predicted for Test Data")
abline(0,1, col="darkorange", lwd=2)
```


Checking RMSE:

```{r}
(rmse = sqrt(mean((predicted_abv - actual_abv)^2)))
```

Basically, if we are to assume for a moment that the test data all come from our attempts to brew beer, it would seem that by following the measuring gadgets ( or sensors ) based on the preferred model 3, it can be observed that a standard error of  `r rmse` seems enough to predict the expected alcohol content without other extraneous sensors - and to rely for more additional sensors does not give additional benefit or effect (see R-squared/RMSE) (though statistically, those extraneous sensors may be significant by ANOVA). It should be noted that based on the Exhaustive search (model 3), for ABV (alcohol by volume), by way of parameterization (sort of brute force), it suggest to have a good mix of either "extract" or "BIAB" for BrewMethod along with "Specific Gravity" for SugarScale. The interaction between Color and BoilTime, Boltime and Brewmethod of 'extract'. (This could very well be done also by subsetting dataset to those observations only)

In other words, it is fair to say - referring to some of the extraneous variables - that we don't have to measure the interaction between Color and BoilGravity, or between Color and Efficiency as determined by model 1 because those interactions do not prove to have additional effect in model 3. (Ref 17.3 of text)

**Other analysis in the context of drinking beers:**

Assume, that we are master at brewing beer at home, and all we want is really to serve up occasional (non-strong) drinkers. We know that those beer drinkers may prefer ABV in the range of 4-6; in other words, the predicted-vs-actual plot show some unusual observations above the 20 or even 30 range. Shall we eliminate them? Let's see.

```{r}
range(actual_abv)
range(predicted_abv)
```

Oh no! We got very strong beer mix out there!  If we are to serve up beer between 4-6 ABVs, what would that proportion be in the true distribution?

```{r}
mean(actual_abv <= 6 & actual_abv >=4 ) 
```

Would that be the same in the emperical distribution?

```{r}
mean(predicted_abv <= 6 & predicted_abv >=4 ) 
```

Close enough! Not bad!



---

### 5.2 Conclusion

A final concluding statement

---

## 6. Appendix

---

### 6.1 Appendix - A

Some graphs to point out.

This is the result of the dataset after outliers are removed (whether model1, model2, model3).

Residual vs Fitted Plot:

```{r}
data_no_outliers =  residual_fitted_plot(model3, new_train_set, FALSE) 
```

Notice that residual range dropped from (-19, 6) to (-6, to 6)

Normal Q-Q Plot:

```{r}
# Let's draw Normal Q-Q plot for the original model3
qqnorm(resid(model3), col = "darkgrey")
qqline(resid(model3), col = "dodgerblue", lwd = 2)
```

Still within the range of the residual range. The residuals are normally distributed (symmetric), but we are not going to put penalty on the outliers ( tails moving away from the blue line).

---

---

### 6.2 Appendix - B

**Just an extra, for fun,  what if we brew our own beer at home, but want to only serve 4-6 ABV contents for occasional beer drinkers?**

Let's see (still using model 3):

```{r}

test_for_occasions = new_test_set[ which ( new_test_set$ABV <= 6 & new_test_set$ABV >=4 ),]

predicted_occasions = predict(model3, test_for_occasions, type="response")
actual_occasions = test_for_occasions$ABV

# RMSE
(rmse = sqrt(mean((predicted_occasions - actual_occasions)^2)))

```

That's so much better. We should be able to brew beer, predicting an expected alcohool content with an error of only `r rmse` if we focus our beer making around the 4-6 ABV.

---

### 6.3 Appendix - C

**Can we serve drinkers with advance taste for beer?**

Here, we're assuming they're looking for stronger beer above 6 ABV perhaps. Let's see:

```{r}

test_for_hard = new_test_set[ which ( new_test_set$ABV > 6 ),]

predicted_hard = predict(model3, test_for_hard, type="response")
actual_hard = test_for_hard$ABV

# RMSE
(rmse = sqrt(mean((predicted_hard - actual_hard)^2)))

```

Probably not. We will have an error rate of `r rmse` - those beer drinkers will not be happy.

---
